{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43846405",
   "metadata": {},
   "source": [
    "# Performance Optimization in Large Datasets\n",
    "\n",
    "### What is Performance Optimization in Large Datasets?\n",
    "\n",
    "Performance optimization in the context of large datasets refers to **techniques and strategies used to efficiently handle, process, and visualize data** that is too big to work with naively. When datasets contain **millions of rows or hundreds of columns**, standard operations like plotting, aggregating, or computing statistics can become **slow, memory-intensive, or even crash our system**.\n",
    "\n",
    "Optimization involves **reducing computational overhead**, **using memory efficiently**, and **leveraging tools that handle large-scale data** effectively. This is critical in **data analysis, AI/ML, and real-time applications**, where responsiveness and speed are essential.\n",
    "\n",
    "Key strategies include:\n",
    "\n",
    "- **Data Sampling:** Working with a representative subset instead of the full dataset.\n",
    "- **Vectorized Operations:** Using libraries like **NumPy** and **Pandas** to replace Python loops.\n",
    "- **Efficient Data Types:** Using `category` for strings, smaller integer/floats to save memory.\n",
    "- **Chunk Processing:** Reading or processing data in small portions instead of loading everything at once.\n",
    "- **Optimized Visualization:** Plotting only necessary data points or using faster plotting libraries like **Datashader** or **Plotly**.\n",
    "\n",
    "### Why is this Important?\n",
    "\n",
    "- Large datasets are common in **finance, e-commerce, IoT, and AI/ML** projects.\n",
    "- Efficient processing saves **time and system resources**.\n",
    "- Optimized code ensures **scalability** â€” the same logic can work as datasets grow.\n",
    "- Helps in **real-time analytics** where data is constantly updating.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "1. Using Efficient Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/Superstore.csv\", encoding='latin-1')\n",
    "\n",
    "# Check memory usage\n",
    "print(df.info(memory_usage=\"deep\"))\n",
    "\n",
    "# Optimize datatypes\n",
    "df[\"Category\"] = df[\"Category\"].astype(\"category\")\n",
    "df[\"Sub-Category\"] = df[\"Sub-Category\"].astype(\"category\")\n",
    "df[\"Order ID\"] = df[\"Order ID\"].astype(\"string\")\n",
    "df[\"Quantity\"] = df[\"Quantity\"].astype(\"int16\")\n",
    "df[\"Sales\"] = df[\"Sales\"].astype(\"float32\")\n",
    "\n",
    "print(df.info(memory_usage=\"deep\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34483493",
   "metadata": {},
   "source": [
    "2. Vectorized Operations instead of Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow loop approach\n",
    "profit_ratio = []\n",
    "for sale, profit in zip(df[\"Sales\"], df[\"Profit\"]):\n",
    "    profit_ratio.append(profit / sale)\n",
    "\n",
    "# Optimized vectorized approach\n",
    "profit_ratio_vec = df[\"Profit\"] / df[\"Sales\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d296ed",
   "metadata": {},
   "source": [
    "3. Processing in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "total_sales = 0\n",
    "\n",
    "for chunk in pd.read_csv(\"large_dataset.csv\", chunksize=chunksize):\n",
    "    total_sales += chunk[\"Sales\"].sum()\n",
    "\n",
    "print(\"Total Sales:\", total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798907c0",
   "metadata": {},
   "source": [
    "4. Sampling for Fast Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(frac=0.05, random_state=42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(sample_df[\"Sales\"], sample_df[\"Profit\"], alpha=0.5)\n",
    "plt.xlabel(\"Sales\")\n",
    "plt.ylabel(\"Profit\")\n",
    "plt.title(\"Sales vs Profit (Sampled Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765dbb9b",
   "metadata": {},
   "source": [
    "5. Using Aggregation for Large Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_by_category = df.groupby(\"Category\")[\"Sales\"].sum().reset_index()\n",
    "print(sales_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5c56c",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Q1. Load a large CSV in chunks and calculate total Profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fcfd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "total_profit = 0\n",
    "for chunk in pd.read_csv(\"large_dataset.csv\", chunksize=chunksize):\n",
    "    total_profit += chunk[\"Profit\"].sum()\n",
    "print(\"Total Profit:\", total_profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d4ee3",
   "metadata": {},
   "source": [
    "Q2. Convert string columns to `category` and check memory reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"] = df[\"Category\"].astype(\"category\")\n",
    "df[\"Sub-Category\"] = df[\"Sub-Category\"].astype(\"category\")\n",
    "print(df.info(memory_usage=\"deep\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca10ea",
   "metadata": {},
   "source": [
    "Q3. Sample 10% of data and plot Sales vs Discount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a334765",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(frac=0.1, random_state=42)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(sample_df[\"Sales\"], sample_df[\"Discount\"], alpha=0.5)\n",
    "plt.xlabel(\"Sales\")\n",
    "plt.ylabel(\"Discount\")\n",
    "plt.title(\"Sales vs Discount (Sampled)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90631d2e",
   "metadata": {},
   "source": [
    "Q4. Use vectorized operations to compute Profit Ratio (Profit/Sales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1878db",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_ratio = df[\"Profit\"] / df[\"Sales\"]\n",
    "print(profit_ratio.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23b3ca1",
   "metadata": {},
   "source": [
    "Q5. Aggregate total Quantity by Region and Category efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168497b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = df.groupby([\"Region\", \"Category\"])[\"Quantity\"].sum().reset_index()\n",
    "print(agg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8b6fe",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Performance optimization in large datasets ensures that **data analysis and visualization remain fast, efficient, and scalable**. By using techniques like **data sampling, vectorized operations, optimized data types, chunk processing, and aggregation**, we can handle millions of rows without crashing the system. This is crucial in **AI/ML pipelines, real-time analytics, and business intelligence dashboards**, where both **speed and memory efficiency** matter. Sampling and aggregation allow analysts to **explore trends quickly**, while vectorized operations and optimized types reduce computational overhead. Ultimately, mastering performance optimization allows us to **work confidently with large-scale datasets**, extract insights faster, and build scalable data-driven solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
