{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6470610f",
   "metadata": {},
   "source": [
    "# Pandas for Machine Learning Preparation\n",
    "\n",
    "Before building a Machine Learning (ML) model, raw data must be transformed into a **model-friendly format**. Pandas plays a crucial role in this phase, providing tools for **cleaning, transforming, and organizing datasets** to maximize model accuracy. In the Titanic dataset, preparation means ensuring that features are numerical (or properly encoded), scaled, and free from inconsistencies.\n",
    "\n",
    "**1. Feature Selection**\n",
    "\n",
    "Not all columns are useful for prediction. For example, `PassengerId`, `Name`, and `Ticket` may not directly help predict survival, so we drop them to reduce noise.\n",
    "\n",
    "**2. Encoding Categorical Variables**\n",
    "\n",
    "ML algorithms canâ€™t process text directly. Pandas makes encoding easy via `pd.get_dummies()` or `LabelEncoder` to transform columns like `Sex` and `Embarked` into numeric form.\n",
    "\n",
    "**3. Handling Missing Data**\n",
    "\n",
    "We fill or drop missing values in columns like `Age` and `Cabin`. Filling with the median or a placeholder avoids model bias.\n",
    "\n",
    "**4. Feature Scaling**\n",
    "\n",
    "Some algorithms (like Logistic Regression, SVM, and KNN) perform better when features are on a similar scale. Pandas works with Scikit-learn scalers to standardize or normalize columns like `Fare` and `Age`.\n",
    "\n",
    "**5. Train-Test Split Preparation**\n",
    "\n",
    "The cleaned and transformed DataFrame is split into `X` (features) and `y` (target variable, `Survived`), ready for training.\n",
    "\n",
    "### Example with Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98eae430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Shape: (891, 12)\n",
      "\n",
      "First 5 rows of the original dataset:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Shape after dropping irrelevant columns: (891, 9)\n",
      "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked\n",
      "0         0       3    male  22.0      1      0   7.2500   NaN        S\n",
      "1         1       1  female  38.0      1      0  71.2833   C85        C\n",
      "2         1       3  female  26.0      0      0   7.9250   NaN        S\n",
      "3         1       1  female  35.0      1      0  53.1000  C123        S\n",
      "4         0       3    male  35.0      0      0   8.0500   NaN        S\n",
      "\n",
      "Columns after one-hot encoding:\n",
      "Index(['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin',\n",
      "       'Sex_male', 'Embarked_Q', 'Embarked_S'],\n",
      "      dtype='object')\n",
      "\n",
      "Missing values after imputation:\n",
      "Survived      0\n",
      "Pclass        0\n",
      "Age           0\n",
      "SibSp         0\n",
      "Parch         0\n",
      "Fare          0\n",
      "Cabin         0\n",
      "Sex_male      0\n",
      "Embarked_Q    0\n",
      "Embarked_S    0\n",
      "dtype: int64\n",
      "\n",
      "First 5 rows after scaling Age and Fare:\n",
      "        Age      Fare\n",
      "0  0.271174  0.014151\n",
      "1  0.472229  0.139136\n",
      "2  0.321438  0.015469\n",
      "3  0.434531  0.103644\n",
      "4  0.434531  0.015713\n",
      "\n",
      "Feature matrix shape: (891, 9)\n",
      "Target vector shape: (891,)\n",
      "\n",
      "First 5 rows of features:\n",
      "   Pclass       Age  SibSp  Parch      Fare    Cabin  Sex_male  Embarked_Q  \\\n",
      "0       3  0.271174      1      0  0.014151  Unknown      True       False   \n",
      "1       1  0.472229      1      0  0.139136      C85     False       False   \n",
      "2       3  0.321438      0      0  0.015469  Unknown     False       False   \n",
      "3       1  0.434531      1      0  0.103644     C123     False       False   \n",
      "4       3  0.434531      0      0  0.015713  Unknown      True       False   \n",
      "\n",
      "   Embarked_S  \n",
      "0        True  \n",
      "1       False  \n",
      "2        True  \n",
      "3        True  \n",
      "4        True  \n",
      "\n",
      "First 5 target values:\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "print(\"Initial Data Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows of the original dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\n",
    "print(\"\\nShape after dropping irrelevant columns:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "print(\"\\nColumns after one-hot encoding:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Fill missing values\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Cabin'] = df['Cabin'].fillna('Unknown')\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])\n",
    "print(\"\\nFirst 5 rows after scaling Age and Fare:\")\n",
    "print(df[['Age', 'Fare']].head())\n",
    "\n",
    "# Split into features and target\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "print(\"\\nFeature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)\n",
    "print(\"\\nFirst 5 rows of features:\")\n",
    "print(X.head())\n",
    "print(\"\\nFirst 5 target values:\")\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c65b362",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Pandas bridges the gap between raw datasets and ML-ready data. By **selecting relevant features, encoding categorical variables, handling missing values, and scaling numerical data**, you ensure your ML algorithm receives clean, consistent input. This preparation step directly impacts model accuracy, stability, and training speed. Without it, even the most advanced algorithm can underperform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
